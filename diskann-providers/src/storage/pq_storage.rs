/*
 * Copyright (c) Microsoft Corporation.
 * Licensed under the MIT license.
 */
use std::io::Write;

use super::{StorageReadProvider, StorageWriteProvider};
use diskann::{
    ANNError, ANNResult,
    utils::{IntoUsize, VectorRepr},
};
use rand::Rng;
use tracing::info;

use crate::{
    model::{
        FixedChunkPQTable, NUM_PQ_CENTROIDS, PQCompressedData,
        pq::{METADATA_SIZE, convert_types},
    },
    utils::{
        copy_aligned_data, gen_random_slice, load_bin, save_bin_f32, save_bin_u32, save_bin_u64,
        write_metadata,
    },
};

// Create types to make return values easier to understand
type FullPivotDataType = Vec<f32>;
type CentroidType = Vec<f32>;
type ChunkOffsetsType = Vec<usize>;
type OpqRotationMatrixType = Vec<f32>;

#[derive(Debug, Clone)]
pub struct PQStorage {
    /// Pivot table path
    pivot_data_path: String,

    /// Compressed pivot path
    compressed_data_path: String,

    /// Data stream used to construct PQ table and PQ compressed table.  If PQStorage is used
    /// for reading then this can be None
    data_path: Option<String>,
}

impl PQStorage {
    pub fn new(pivot_data_path: &str, compressed_data_path: &str, data_path: Option<&str>) -> Self {
        Self {
            pivot_data_path: pivot_data_path.to_string(),
            compressed_data_path: compressed_data_path.to_string(),
            data_path: data_path.map(|x| x.to_string()),
        }
    }

    pub fn write_compressed_pivot_metadata<Storage>(
        &self,
        npts: usize,
        pq_chunk: usize,
        writer: &mut Storage::Writer,
    ) -> ANNResult<()>
    where
        Storage: StorageWriteProvider,
    {
        write_metadata(writer, npts, pq_chunk)?;
        Ok(())
    }

    /// Write the pivot table to file
    /// # Arguments
    /// * `full_pivot_data` - the pivot table data
    /// * `centroid` - the centroid of the pivot table
    /// * `chunk_offsets` - the chunk offsets of the pivot table
    /// * `num_centers` - the number of centers
    /// * `dim` - the dimension of the pivot table
    /// * `storage_provider` - the storage provider
    /// # Return
    /// * `Result` - the result of writing the pivot table
    /// # Remarks
    /// * 4k bytes are reserved for metadata at the beginning of the file
    /// * the metadata is written in the following order:
    ///     * the size of the metadata
    ///     * the offset of the pivot table data
    ///     * the offset of the centroid of the pivot table
    ///     * the offset of the chunk offsets of the pivot table
    /// * the pivot table data: num_centers * dim
    /// * the centroid of the pivot table: dim*1
    /// * the chunk offsets of the pivot table: (num_pq_chunks) + 1 * 1
    pub fn write_pivot_data<Storage>(
        &self,
        full_pivot_data: &[f32],
        centroid: &[f32],
        chunk_offsets: &[usize],
        num_centers: usize,
        dim: usize,
        storage_provider: &Storage,
    ) -> ANNResult<()>
    where
        Storage: StorageWriteProvider,
    {
        let mut cumul_bytes: Vec<usize> = vec![0; 4];
        cumul_bytes[0] = METADATA_SIZE;
        let writer = &mut storage_provider.create_for_write(&self.pivot_data_path)?;
        // Write Pq centroids vectors at offset METADATA_SIZE(4096)
        cumul_bytes[1] = cumul_bytes[0]
            + save_bin_f32(writer, full_pivot_data, num_centers, dim, cumul_bytes[0])?;

        // Write THE CENTROID of PQ CENTROID vectors at offset METADATA_SIZE(4096) + size of centroids vectors.
        cumul_bytes[2] = cumul_bytes[1] + save_bin_f32(writer, centroid, dim, 1, cumul_bytes[1])?;

        // Because the writer only can write u32, u64 but not usize, so we need to convert the type first.
        let chunk_offsets_u32 =
            convert_types(chunk_offsets, chunk_offsets.len(), |x: usize| x as u32);

        // Write PQ chunk offsets at offset METADATA_SIZE(4096) + size of PQ centroids vectors + size of the centroid vector.
        cumul_bytes[3] = cumul_bytes[2]
            + save_bin_u32(
                writer,
                &chunk_offsets_u32,
                chunk_offsets_u32.len(),
                1,
                cumul_bytes[2],
            )?;

        // Write metadata at offset 0.
        let cumul_bytes_u64 = convert_types(&cumul_bytes, cumul_bytes.len(), |x: usize| x as u64);
        save_bin_u64(writer, &cumul_bytes_u64, cumul_bytes_u64.len(), 1, 0)?;

        writer.flush()?;
        Ok(())
    }

    fn get_rotation_matrix_path(&self) -> String {
        format!("{}_rotation_matrix.bin", self.pivot_data_path)
    }

    /// Writes the rotation matrix generated by tho Optimized Product Quantization algorithm to a file.
    /// # Arguments
    /// * `rotation_matrix` - matrix to write out.  Matrix should be stored in memory as a single array
    /// * `dimension` - Number of points and number of columns in the matrix.  For this matrix, these are the same.
    /// * `storage_provider` - Storage provider to use when writing out data
    /// # Return
    /// * `Result` - the result of writing the pivot table
    pub fn write_rotation_matrix_data<Storage>(
        &self,
        rotation_matrix: &[f32],
        dimension: usize,
        storage_provider: &Storage,
    ) -> ANNResult<()>
    where
        Storage: StorageWriteProvider,
    {
        let writer = &mut storage_provider.create_for_write(&self.get_rotation_matrix_path())?;

        // Save the rotation matrix
        save_bin_f32(writer, rotation_matrix, dimension, dimension, 0)?;

        Ok(())
    }

    pub fn pivot_data_exist<Storage>(&self, storage_provider: &Storage) -> bool
    where
        Storage: StorageReadProvider,
    {
        storage_provider.exists(&self.pivot_data_path)
    }

    pub fn read_existing_pivot_metadata<Storage>(
        &self,
        storage_provider: &Storage,
    ) -> std::io::Result<(usize, usize)>
    where
        Storage: StorageReadProvider,
    {
        let reader = &mut storage_provider.open_reader(&self.pivot_data_path)?;
        let (_, file_num_centers, file_dim) =
            load_bin::<f32, Storage::Reader>(reader, METADATA_SIZE)?;
        Ok((file_num_centers, file_dim))
    }

    pub fn load_existing_pivot_data<Storage>(
        &self,
        num_pq_chunks: &usize,
        num_centers: &usize,
        dim: &usize,
        storage_provider: &Storage,
        use_opq: bool,
    ) -> ANNResult<(
        FullPivotDataType,
        CentroidType,
        ChunkOffsetsType,
        OpqRotationMatrixType,
    )>
    where
        Storage: StorageReadProvider,
    {
        // Load file offset data. File saved as offset data(4*1) -> pivot data(centroid num*dim) -> centroid of dim data(dim*1) -> chunk offset data(chunksize+1*1)
        // Because we only can write u64 rather than usize, so the file stored as u64 type. Need to convert to usize when use.
        let reader = &mut storage_provider.open_reader(&self.pivot_data_path)?;

        let (data, offset_num, nc) = load_bin::<u64, Storage::Reader>(reader, 0)?;
        let file_offset_data = convert_types(&data, offset_num * nc, |x: u64| x.into_usize());
        if offset_num != 4 {
            return Err(ANNError::log_pq_error(format_args!(
                "Error reading pq_pivots file {}. Offsets don't contain correct \
                 metadata, # offsets = {}, but expecting 4.",
                &self.pivot_data_path, offset_num
            )));
        }

        info!(" Offset data: {:?}", file_offset_data);

        let (data, pivot_num, pivot_dim) =
            load_bin::<f32, Storage::Reader>(reader, file_offset_data[0])?;
        let full_pivot_data = data;
        if pivot_num != *num_centers || pivot_dim != *dim {
            return Err(ANNError::log_pq_error(format_args!(
                "Error reading pq_pivots file {}. file_num_centers = {}, \
                 file_dim = {} but expecting {} centers in {} dimensions.",
                &self.pivot_data_path, pivot_num, pivot_dim, num_centers, dim
            )));
        }

        let (data, centroid_dim, nc) =
            load_bin::<f32, Storage::Reader>(reader, file_offset_data[1])?;
        let centroid = data;
        if centroid_dim != *dim || nc != 1 {
            return Err(ANNError::log_pq_error(format_args!(
                "Error reading pq_pivots file {}. file_dim = {}, \
                 file_cols = {} but expecting {} entries in 1 dimension.",
                &self.pivot_data_path, centroid_dim, nc, dim
            )));
        }

        let (data, chunk_offset_number, nc) =
            load_bin::<u32, Storage::Reader>(reader, file_offset_data[2])?;
        let chunk_offsets = convert_types(&data, chunk_offset_number * nc, |x: u32| x.into_usize());
        if chunk_offset_number != *num_pq_chunks + 1 || nc != 1 {
            return Err(ANNError::log_pq_error(format_args!(
                "Error reading pq_pivots file at chunk offsets; \
                file has nr={}, nc={} but expecting nr={} and nc=2.",
                chunk_offset_number,
                nc,
                num_pq_chunks + 1
            )));
        }

        let opq_rotation_matrix = if use_opq {
            self.read_opq_rotation_matrix(storage_provider)?
        } else {
            // Create empty vector
            vec![]
        };

        Ok((
            full_pivot_data,
            centroid,
            chunk_offsets,
            opq_rotation_matrix,
        ))
    }

    /// Read the OPQ matrix from storage.  This method assumes that the OPQ rotation matrix exists.
    /// If the matrix does not exist then an error is returned.
    ///
    /// # Arguments
    /// `storage_provider` - Storage object that provides readers.
    fn read_opq_rotation_matrix<Storage: StorageReadProvider>(
        &self,
        storage_provider: &Storage,
    ) -> Result<Vec<f32>, ANNError> {
        let rotation_matrix_filepath = self.get_rotation_matrix_path();
        let open_reader_result = storage_provider.open_reader(&rotation_matrix_filepath);
        let rotation_matrix_reader = &mut match open_reader_result {
            Ok(reader) => reader,
            // Make it clear what file we were trying to open when the error occurred and return the error
            Err(e) => {
                return Err(ANNError::log_opq_error(format!(
                    "Unable to open reader for OPQ rotation matrix file '{}'.  Inner Error: '{}'",
                    rotation_matrix_filepath, e
                )));
            }
        };

        let (data, _, _) = load_bin::<f32, Storage::Reader>(rotation_matrix_reader, 0)?;

        info!("OPQ rotation matrix load complete");

        Ok(data)
    }

    /// Load the compressed pq dataset from file
    pub fn load_pq_compressed_vectors_bin<Storage: StorageReadProvider>(
        pq_compressed_data: &str,
        num_points_to_load: usize,
        num_pq_chunks: usize,
        storage_provider: &Storage,
    ) -> ANNResult<PQCompressedData> {
        info!(
            "Loading compressed from pq compressed data file {}...",
            pq_compressed_data,
        );

        info!(
            "# of Points: {} , # PQ chunks: {} ",
            num_points_to_load, num_pq_chunks
        );

        let mut pq_compressed_dataset = PQCompressedData::new(num_points_to_load, num_pq_chunks)?;

        let (_, _) = copy_aligned_data(
            &mut storage_provider.open_reader(pq_compressed_data)?,
            pq_compressed_dataset.into_dto(),
            0,
        )?;
        info!("PQ compressed dataset loaded.");
        Ok(pq_compressed_dataset)
    }

    /// Load pre-trained pivot table
    pub fn load_pq_pivots_bin<Storage: StorageReadProvider>(
        &self,
        pq_pivots: &str,
        num_pq_chunks: usize,
        storage_provider: &Storage,
    ) -> ANNResult<FixedChunkPQTable> {
        if !storage_provider.exists(pq_pivots) {
            return Err(ANNError::log_pq_error(
                "ERROR: PQ k-means pivot file not found.",
            ));
        }

        info!("Loading PQ pivots from {}...", pq_pivots);

        let mut reader = storage_provider.open_reader(pq_pivots)?;
        let (data, offset_num, offset_dim) = load_bin::<u64, Storage::Reader>(&mut reader, 0)?;
        let file_offset_data =
            convert_types(&data, offset_num * offset_dim, |x: u64| x.into_usize());
        if offset_num != 4 {
            return Err(ANNError::log_pq_error(format_args!(
                "Error reading pq_pivots file {}. Offsets don't contain correct metadata, \
                 # offsets = {}, but expecting 4.",
                pq_pivots, offset_num
            )));
        }

        let (data, pivot_num, dim) =
            load_bin::<f32, Storage::Reader>(&mut reader, file_offset_data[0])?;
        let pq_table = data.to_vec();
        if pivot_num > NUM_PQ_CENTROIDS {
            return Err(ANNError::log_pq_error(format_args!(
                "Error reading pq_pivots file {}. file_num_centers = {}, but expecting {} centers.",
                pq_pivots, pivot_num, NUM_PQ_CENTROIDS
            )));
        }

        let (data, centroid_dim, nc) =
            load_bin::<f32, Storage::Reader>(&mut reader, file_offset_data[1])?;
        let centroids = data.to_vec();
        if centroid_dim != dim || nc != 1 {
            return Err(ANNError::log_pq_error(format_args!(
                "Error reading pq_pivots file {}. file_dim = {}, file_cols = {} \
                 but expecting {} entries in 1 dimension.",
                pq_pivots, centroid_dim, nc, dim
            )));
        }

        let (data, chunk_offset_num, nc) =
            load_bin::<u32, Storage::Reader>(&mut reader, file_offset_data[2])?;
        let chunk_offsets = convert_types(&data, chunk_offset_num * nc, |x: u32| x.into_usize());
        if (chunk_offset_num != num_pq_chunks + 1 && num_pq_chunks as u32 != 0) || nc != 1 {
            return Err(ANNError::log_pq_error(format_args!(
                "Error reading pq_pivots file at chunk offsets; file has nr={}, nc={} \
                 but expecting nr={} and nc=1. The expected num_pq_chunks should be \
                 passed as 0 if we want to infer.",
                chunk_offset_num,
                nc,
                num_pq_chunks + 1
            )));
        }

        let opq_rotation_matrix: Option<Box<[f32]>> =
            if storage_provider.exists(&self.get_rotation_matrix_path()) {
                // Convert Result to Option
                Some(self.read_opq_rotation_matrix(storage_provider)?.into())
            } else {
                None
            };

        FixedChunkPQTable::new(
            dim,
            pq_table.into(),
            centroids.into(),
            chunk_offsets.into(),
            opq_rotation_matrix,
        )
    }

    /// streams data from the file, and samples each vector with probability p_val
    /// and returns a matrix of size slice_size* ndims as floating point type.
    /// the slice_size and ndims are set inside the function.
    /// # Arguments
    /// * `file_name` - filename where the data is
    /// * `p_val` - possibility to sample data
    /// * `sampled_vectors` - sampled vector chose by p_val possibility
    /// * `slice_size` - how many sampled data return
    /// * `dim` - each sample data dimension
    pub fn get_random_train_data_slice<T: VectorRepr, Storage>(
        &self,
        p_val: f64,
        storage_provider: &Storage,
        generator: &mut impl Rng,
    ) -> ANNResult<(Vec<f32>, usize, usize)>
    where
        Storage: StorageReadProvider,
    {
        gen_random_slice::<T, _>(self.get_data_path()?, p_val, storage_provider, generator)
    }

    pub fn get_data_path(&self) -> ANNResult<&str> {
        self.data_path
            .as_ref()
            .ok_or_else(|| {
                ANNError::log_index_config_error(
                    "data_path".to_string(),
                    "pq_storage.data_path is not defined".to_string(),
                )
            })
            .map(|s| s.as_str())
    }

    pub fn get_compressed_data_path(&self) -> &str {
        &self.compressed_data_path
    }
}

#[cfg(test)]
mod pq_storage_tests {

    use crate::storage::VirtualStorageProvider;
    use diskann_utils::test_data_root;
    use vfs::MemoryFS;

    use super::*;
    use crate::utils::{gen_random_slice, read_metadata};

    const DATA_FILE: &str = "/sift/siftsmall_learn.bin";
    const PQ_PIVOT_PATH: &str = "/sift/siftsmall_learn_pq_pivots.bin";
    const PQ_COMPRESSED_PATH: &str = "/sift/empty_pq_compressed.bin";

    #[test]
    fn new_test() {
        PQStorage::new(PQ_PIVOT_PATH, PQ_COMPRESSED_PATH, Some(DATA_FILE));
    }

    #[test]
    fn write_compressed_pivot_metadata_test() {
        let filesystem = MemoryFS::new();
        let storage_provider = VirtualStorageProvider::new(filesystem);
        let compress_pivot_path = "/write_compressed_pivot_metadata_test.bin";
        let result = PQStorage::new(PQ_PIVOT_PATH, compress_pivot_path, Some(DATA_FILE));
        {
            let mut writer = storage_provider
                .create_for_write(compress_pivot_path)
                .unwrap();

            result
                .write_compressed_pivot_metadata::<VirtualStorageProvider<MemoryFS>>(
                    100,
                    20,
                    &mut writer,
                )
                .unwrap();
        }

        let mut result_reader = storage_provider.open_reader(compress_pivot_path).unwrap();
        let metadata = read_metadata(&mut result_reader).unwrap();

        assert_eq!(metadata.npoints, 100);
        assert_eq!(metadata.ndims, 20);

        storage_provider.delete(compress_pivot_path).unwrap();
    }

    #[test]
    fn pivot_data_exist_test() {
        let storage_provider = VirtualStorageProvider::new_overlay(test_data_root());
        let result = PQStorage::new(PQ_PIVOT_PATH, PQ_COMPRESSED_PATH, Some(DATA_FILE));
        assert!(result.pivot_data_exist(&storage_provider));

        let pivot_path = "not_exist_pivot_path.bin";
        let result = PQStorage::new(pivot_path, PQ_COMPRESSED_PATH, Some(DATA_FILE));
        assert!(!result.pivot_data_exist(&storage_provider));
    }

    #[test]
    fn read_pivot_metadata_test() {
        let storage_provider = VirtualStorageProvider::new_overlay(test_data_root());
        let result = PQStorage::new(PQ_PIVOT_PATH, PQ_COMPRESSED_PATH, Some(DATA_FILE));
        let (npt, dim) = result
            .read_existing_pivot_metadata(&storage_provider)
            .unwrap();

        assert_eq!(npt, 256);
        assert_eq!(dim, 128);
    }

    #[test]
    fn load_pivot_data_test() {
        let storage_provider = VirtualStorageProvider::new_overlay(test_data_root());
        let result = PQStorage::new(PQ_PIVOT_PATH, PQ_COMPRESSED_PATH, Some(DATA_FILE));
        let (pq_pivot_data, centroids, chunk_offsets, _) = result
            .load_existing_pivot_data(&1, &256, &128, &storage_provider, false)
            .unwrap();

        assert_eq!(pq_pivot_data.len(), 256 * 128);
        assert_eq!(centroids.len(), 128);
        assert_eq!(chunk_offsets.len(), 2);
    }

    #[test]
    fn load_pivot_data_with_opq_test() {
        // OPQ matrices are square
        const OPQ_MATRIX_SIZE: usize = 128 * 128;

        // Create dummy OPQ matrix with test data
        let dummy_opq_matrix = vec![1.345; OPQ_MATRIX_SIZE];

        let storage_provider = VirtualStorageProvider::new_overlay(test_data_root());
        let pq_storage = PQStorage::new(PQ_PIVOT_PATH, PQ_COMPRESSED_PATH, Some(DATA_FILE));

        // Write OPQ test data
        pq_storage
            .write_rotation_matrix_data(&dummy_opq_matrix, 128, &storage_provider)
            .expect("Failed to save OPQ test data");

        let (pq_pivot_data, centroids, chunk_offsets, opq_rotation_matrix) = pq_storage
            .load_existing_pivot_data(&1, &256, &128, &storage_provider, true)
            .unwrap();

        assert_eq!(pq_pivot_data.len(), 256 * 128);
        assert_eq!(centroids.len(), 128);
        assert_eq!(chunk_offsets.len(), 2);

        // Check Optimized Product Quantization matrix
        assert_eq!(opq_rotation_matrix.len(), OPQ_MATRIX_SIZE);
        assert_eq!(opq_rotation_matrix, dummy_opq_matrix);

        storage_provider
            .delete(&pq_storage.get_rotation_matrix_path())
            .expect("Failed to delete OPQ temp");
    }

    #[test]
    fn gen_random_slice_test() {
        let filesystem = MemoryFS::new();
        let storage_provider = VirtualStorageProvider::new(filesystem);
        let file_name = "/gen_random_slice_test.bin";
        //npoints=2, dim=8
        let data: [u8; 72] = [
            2, 0, 0, 0, 8, 0, 0, 0, 0x00, 0x00, 0x80, 0x3f, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00,
            0x40, 0x40, 0x00, 0x00, 0x80, 0x40, 0x00, 0x00, 0xa0, 0x40, 0x00, 0x00, 0xc0, 0x40,
            0x00, 0x00, 0xe0, 0x40, 0x00, 0x00, 0x00, 0x41, 0x00, 0x00, 0x10, 0x41, 0x00, 0x00,
            0x20, 0x41, 0x00, 0x00, 0x30, 0x41, 0x00, 0x00, 0x40, 0x41, 0x00, 0x00, 0x50, 0x41,
            0x00, 0x00, 0x60, 0x41, 0x00, 0x00, 0x70, 0x41, 0x00, 0x00, 0x80, 0x41,
        ];
        {
            let mut writer = storage_provider.create_for_write(file_name).unwrap();
            writer
                .write_all(&data)
                .expect("Failed to write sample file");
        }

        let (sampled_vectors, slice_size, ndims) =
            gen_random_slice::<f32, VirtualStorageProvider<MemoryFS>>(
                file_name,
                1f64,
                &storage_provider,
                &mut crate::utils::create_rnd_in_tests(),
            )
            .unwrap();
        let mut start = 8;
        (0..sampled_vectors.len()).for_each(|i| {
            assert_eq!(sampled_vectors[i].to_le_bytes(), data[start..start + 4]);
            start += 4;
        });
        assert_eq!(sampled_vectors.len(), 16);
        assert_eq!(slice_size, 2);
        assert_eq!(ndims, 8);

        let (sampled_vectors, slice_size, ndims) =
            gen_random_slice::<f32, VirtualStorageProvider<MemoryFS>>(
                file_name,
                0f64,
                &storage_provider,
                &mut crate::utils::create_rnd_in_tests(),
            )
            .unwrap();
        assert_eq!(sampled_vectors.len(), 0);
        assert_eq!(slice_size, 0);
        assert_eq!(ndims, 8);

        storage_provider
            .delete(file_name)
            .expect("Failed to delete file");
    }
}
